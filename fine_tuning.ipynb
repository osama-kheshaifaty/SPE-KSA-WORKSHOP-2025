{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIWDisMIAyhgP5h1agjl63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osama-kheshaifaty/SPE-KSA-WORKSHOP-2025/blob/main/fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLeMIXwMVrBk"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning a Pretrained LLM (DistilBERT) on a Small Text Classification Task\n",
        "# -------------------------------------------------------------------------------\n",
        "\n",
        "# Step 1: Install required libraries\n",
        "# Uncomment and run this line if you're missing the libraries\n",
        "# !pip install transformers datasets scikit-learn\n",
        "\n",
        "# Step 2: Import libraries\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Step 3: Prepare a Small Synthetic Dataset\n",
        "# --------------------------------------------\n",
        "# We'll create a simple dataset: classify text as positive (1) or negative (0)\n",
        "\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"The well production increased significantly\",\n",
        "        \"We encountered severe water breakthrough\",\n",
        "        \"The ESP failed unexpectedly after 2 months\",\n",
        "        \"Drilling was faster than expected\",\n",
        "        \"Production rate dropped by 40%\",\n",
        "        \"Well intervention improved flow rate\",\n",
        "        \"Unexpected sand production started\",\n",
        "        \"Field redevelopment was successful\",\n",
        "    ],\n",
        "    \"label\": [1, 0, 0, 1, 0, 1, 0, 1]\n",
        "}\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "# Step 4: Load Pre-trained Model and Tokenizer\n",
        "# ----------------------------------------------\n",
        "# Tokenizer turns raw text into model-readable numbers (input_ids, attention_mask)\n",
        "# Model is a DistilBERT (small version of BERT)\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Step 5: Tokenize the Dataset\n",
        "# ------------------------------\n",
        "def preprocess(batch):\n",
        "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
        "\n",
        "# Step 6: Define Training Arguments\n",
        "# -----------------------------------\n",
        "# These control how fine-tuning will happen:\n",
        "# - where outputs are saved\n",
        "# - batch size\n",
        "# - number of epochs (full passes through data)\n",
        "# - learning rate (how fast model updates)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",        # folder for saving results\n",
        "    evaluation_strategy=\"epoch\",   # evaluate after each epoch\n",
        "    learning_rate=2e-5,             # small learning rate (important for fine-tuning)\n",
        "    per_device_train_batch_size=4,  # batch size\n",
        "    num_train_epochs=5,             # small number of epochs (since tiny dataset)\n",
        "    weight_decay=0.01,              # small regularization\n",
        "    logging_dir=\"./logs\",           # folder for logs\n",
        "    logging_steps=5,\n",
        ")\n",
        "\n",
        "# Step 7: Define Trainer\n",
        "# ------------------------\n",
        "# Trainer is HuggingFace's simple way to manage training\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Step 8: Fine-tune the Model\n",
        "# -----------------------------\n",
        "# This is where fine-tuning actually happens.\n",
        "# The model weights will adjust slightly to better fit this new specific task.\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Step 9: Test the Fine-tuned Model\n",
        "# -----------------------------------\n",
        "# Let's manually test a new sentence.\n",
        "\n",
        "test_text = \"The new well intervention was highly successful.\"\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = logits.argmax().item()\n",
        "\n",
        "# Map prediction to label\n",
        "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
        "print(f\"\\nPrediction for: \\\"{test_text}\\\"\")\n",
        "print(\"Predicted label:\", label_map[predicted_class])"
      ]
    }
  ]
}