{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIEow1kFW8WWDTJrWzrMIb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osama-kheshaifaty/SPE-KSA-WORKSHOP-2025/blob/main/reinforecement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgR_Vy7iYXz3"
      },
      "outputs": [],
      "source": [
        "# Simple Reinforcement Learning Example: Balancing a Pole (CartPole)\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# Step 1: Install libraries if missing\n",
        "# Uncomment if running for the first time\n",
        "# !pip install gym\n",
        "\n",
        "# Step 2: Import Required Libraries\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Step 3: Create the Environment\n",
        "# ----------------------------------------------------------\n",
        "# CartPole: A pole is attached to a cart.\n",
        "# Goal: Move the cart left or right to keep the pole balanced.\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Step 4: Define the Q-Learning Agent\n",
        "# ----------------------------------------------------------\n",
        "# Q-Learning is a table-based method where we learn\n",
        "# the best action to take from every possible state.\n",
        "\n",
        "# Note:\n",
        "# - In real CartPole, states are continuous (positions, velocities).\n",
        "# - So for simplicity, we'll **discretize** the states into buckets.\n",
        "\n",
        "n_buckets = (1, 1, 6, 12)  # discretization for each state variable\n",
        "n_actions = env.action_space.n  # 2 actions: left (0), right (1)\n",
        "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
        "state_bounds[1] = [-0.5, 0.5]  # limit cart velocity\n",
        "state_bounds[3] = [-np.radians(50), np.radians(50)]  # limit pole angle rate\n",
        "\n",
        "q_table = np.zeros(n_buckets + (n_actions,))  # initialize Q-table with zeros\n",
        "\n",
        "# Step 5: Define Helper Functions\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "def discretize_state(state):\n",
        "    \"\"\"Convert continuous state into discrete buckets\"\"\"\n",
        "    ratios = [(state[i] + abs(state_bounds[i][0])) / (state_bounds[i][1] - state_bounds[i][0]) for i in range(len(state))]\n",
        "    new_state = [int(round((n_buckets[i] - 1) * ratios[i])) for i in range(len(state))]\n",
        "    new_state = [min(n_buckets[i] - 1, max(0, new_state[i])) for i in range(len(state))]\n",
        "    return tuple(new_state)\n",
        "\n",
        "def choose_action(state, epsilon):\n",
        "    \"\"\"Epsilon-greedy policy: Explore randomly or exploit known best action\"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Explore: random action\n",
        "    else:\n",
        "        return np.argmax(q_table[state])  # Exploit: best known action\n",
        "\n",
        "def update_q(state, action, reward, new_state, alpha, gamma):\n",
        "    \"\"\"Update Q-value based on reward and future expected rewards\"\"\"\n",
        "    best_future_q = np.max(q_table[new_state])\n",
        "    current_q = q_table[state + (action,)]\n",
        "    q_table[state + (action,)] = current_q + alpha * (reward + gamma * best_future_q - current_q)\n",
        "\n",
        "# Step 6: Set Training Parameters\n",
        "# ----------------------------------------------------------\n",
        "n_episodes = 500  # how many games to play\n",
        "alpha = 0.1       # learning rate (how much to trust new info)\n",
        "gamma = 0.99      # discount factor (importance of future rewards)\n",
        "epsilon = 1.0     # initial exploration rate\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "# Step 7: Train the Agent\n",
        "# ----------------------------------------------------------\n",
        "rewards = []\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    current_state = discretize_state(env.reset()[0])\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = choose_action(current_state, epsilon)\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        new_state = discretize_state(obs)\n",
        "\n",
        "        update_q(current_state, action, reward, new_state, alpha, gamma)\n",
        "\n",
        "        current_state = new_state\n",
        "        total_reward += reward\n",
        "\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)  # gradually explore less\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "    if (episode + 1) % 50 == 0:\n",
        "        print(f\"Episode {episode + 1}: Average Reward (last 50) = {np.mean(rewards[-50:]):.2f}\")\n",
        "\n",
        "# Step 8: Evaluate the Trained Agent\n",
        "# ----------------------------------------------------------\n",
        "# After training, let's see how well it performs without exploration.\n",
        "\n",
        "state = discretize_state(env.reset()[0])\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "print(\"\\nWatching the trained agent...\")\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    action = np.argmax(q_table[state])  # always exploit\n",
        "    obs, reward, done, _, _ = env.step(action)\n",
        "    state = discretize_state(obs)\n",
        "    total_reward += reward\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(f\"Total reward achieved: {total_reward}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# End of Simple Reinforcement Learning Example\n",
        "# -----------------------------------------------------------------------------------\n"
      ]
    }
  ]
}